#!/usr/bin/env ruby

# This is the jetpants command suite toolkit executable. It wraps the Jetpants
# module in a Thor command processor, providing a command-line interface to
# common Jetpants functionality.

%w[thor pry pry-rescue highline/import colored].each {|g| require g}

module Jetpants

  class CommandSuite < Thor

    def initialize *args
      super
      # setup pry
      Pry.config.prompt = proc {|object, nest_level, _| "# #{object} > "}
    end

    # Override Thor.dispatch to allow simple callbacks, which must be before_foo /
    # after_foo *class* methods of Jetpants::CommandSuite. 
    # These aren't as full-featured as normal Jetpants::Callback: you can only have
    # ONE before_foo or after_foo method (they override instead of stacking); no arg
    # passing; no callback abort exception type. Mostly useful for plugins overriding
    # reminder text before or after a task.
    def self.dispatch(task, given_args, given_ops, config)
      task_name = task || given_args[0]

      Jetpants.plugins.keys.each do |plugin|
        self.send("#{plugin}_before_dispatch", task_name) if self.respond_to? "#{plugin}_before_dispatch"
        self.send("#{plugin}_before_#{task_name}", task_name, 'before') if self.respond_to? "#{plugin}_before_#{task_name}"
      end

      self.send "before_#{task_name}" if self.respond_to? "before_#{task_name}"
      super
      self.send "after_#{task_name}" if self.respond_to? "after_#{task_name}"
    ensure
      Jetpants.plugins.keys.each do |plugin|
        self.send("#{plugin}_after_dispatch", task_name) if self.respond_to? "#{plugin}_after_dispatch"
        self.send("#{plugin}_after_#{task_name}", task_name, 'after') if self.respond_to? "#{plugin}_after_#{task_name}"
      end
    end

    
    desc 'console', 'Jetpants interactive console'
    def console
      Jetpants.pry
    end
    def self.before_console
      message = [ 'Welcome to the Jetpants Console.  A few notes:',
                  '  - Jetpants interacts with databases via ssh and the root user (BE CAREFUL).',
                  '  - Jetpants relies on having access to a root ssh key in order to perform these operations.',
                  '  - This console operates from inside the Jetpants module namespace by default (Jetpants::DB.new == DB.new, etc).',
                  '  - Jetpants uses a global config file at /etc/jetpants.yaml, or a user override config file at ~/.jetpants.yaml.',
                ].join "\n"
      print "\n#{message}\n\n"
    end
    
    
    desc 'promotion', 'perform a master promotion, changing which node is the master of a pool'
    method_option :demote,  :desc => 'node to demote'
    method_option :promote, :desc => 'node to promote'
    def promotion
      # It's not uncommon for the demoted master to be an offline/unavailable node, so relax Jetpants' normal
      # checks regarding replication threads being in different states.
      Jetpants.verify_replication = false
      
      promoted = options[:promote] ? options[:promote].to_db : nil
      demoted  = options[:demote]  ? options[:demote].to_db  : nil
      
      if promoted && !demoted
        error "Node to promote #{promoted} is not a slave" unless promoted.is_slave?
        demoted = promoted.master
        inform "Will demote #{demoted}, the master of specified promoted node #{promoted}."
      end
      
      if demoted
        demoted.probe
      else
        demoted = ask_node 'Please enter the IP address of the node to demote:'
      end

      if demoted.running?
        error 'Cannot demote a node that has no slaves!' unless demoted.has_slaves?
      else
        inform "Unable to connect to node #{demoted} to demote"
        error  "Unable to perform promotion" unless agree "Please confirm that #{demoted} is offline [yes/no]: "
        
        # An asset-tracker plugin may have been populated the slave list anyway
        if demoted.slaves && demoted.slaves.count > 0
          demoted.slaves.each {|s| s.probe}
        else
          replicas = ask("Please enter a comma-separated list of IP addresses of all current replicas of #{demoted}: ").split /\s*,\s*/
          error "No replicas entered" unless replicas && replicas.count > 0
          error "User supplied list of replicas appears to be invalid - #{replicas}" unless replicas.all? {|replica| is_ip? replica}
          demoted.instance_eval {@slaves = replicas.map &:to_db}
          demoted.slaves.each do |replica|
            # Validate that they are really slaves of demoted
            error "#{replica} does not appear to be a valid replica of #{demoted}" unless replica.master == demoted
          end
        end
      end

      output
      inform "Summary of affected pool"
      inform "Binary log positions and slave lag shown below are just a snapshot taken at the current time." if demoted.running?
      output
      demoted.pool(true).summary(true)
      output

      unless promoted
        if demoted.running?
          inform "Recommendation: promote the standby slave with the highest binary log coordinates"
        else
          inform "Recommendation: promote the standby slave or active slave with the highest binary log coordinates"
        end
        promoted = ask_node 'Please enter the IP address of the node to promote: '
      end
      
      error "Unable to determine a node to demote and a node to promote" unless demoted.kind_of?(Jetpants::DB) && promoted.kind_of?(Jetpants::DB)
      error "Node to promote #{promoted} is not a slave of node to demote #{demoted}" unless promoted.master == demoted
      error "The chosen node cannot be promoted. Please choose another." unless promoted.promotable_to_master?
      
      inform "Going to DEMOTE existing master #{demoted} and PROMOTE new master #{promoted}."
      error "Aborting." unless agree "Proceed? [yes/no]: "
      demoted.pool(true).master_promotion! promoted
    end
    def self.after_promotion
      reminders(
        'Commit/push the configuration in version control.',
        'Deploy the configuration to all machines.'
      )
    end
    
    
    desc 'summary', 'display information about a node in the context of its pool'
    method_option :node, :desc => 'IP address of node to query, or name of pool'
    def summary
      node = options[:node] || ask('Please enter node IP or name of pool: ')
      if is_ip? node
        node = node.to_db
        node.pool(true).probe
        describe node
        node.pool(true).summary(true)
      else
        pool = Jetpants.topology.pool(node)
        raise "#{node} is neither an IP address nor a pool name" unless pool
        pool.probe
        pool.summary(true)
      end
    end
    
    
    desc 'pools', 'display a full summary of every pool in the topology'
    def pools
      Jetpants.functional_partitions.concurrent_each &:probe
      Jetpants.shards.reject {|s| s.parent}.concurrent_each &:probe
      Jetpants.pools.each &:summary
      
      # We could do this more compactly using DB#role, but it would incorrectly
      # double-count nodes involved in a shard split
      counts = {master: 0, active_slave: 0, standby_slave: 0, backup_slave: 0}
      Jetpants.pools.each do |p|
        counts[:master] += 1
        counts[:active_slave] += p.active_slaves.count
        counts[:backup_slave] += p.backup_slaves.count
        counts[:standby_slave] += p.standby_slaves.count
      end

      output
      output "%4d global pools" % Jetpants.functional_partitions.count
      output "%4d shard pools" % Jetpants.shards.count
      output "---- --------------"
      output "%4d total pools" % Jetpants.pools.count
      output
      
      total = 0
      counts.each do |role, count|
        output "%4d %ss" % [count, role.to_s.tr('_', ' ')]
        total += count
      end
      output "---- --------------"
      output "%4d total nodes" % total
      output
    end

    desc 'pools_compact', 'display a compact summary (master, name, and size) of every pool in the topology'
    def pools_compact
      output
      Jetpants.shards.each do |s| 
        output "[%-15s] %8s to %-11s = %4s GB" % [s.ip, s.min_id, s.max_id, s.data_set_size(true)]
      end
      Jetpants.functional_partitions.each do |p| 
        output "[%-15s] %-23s = %4s GB" % [p.ip, p.name, p.data_set_size(true)]
      end
      output
    end
    
    
    desc 'regen_config', 'regenerate the application configuration'
    def regen_config
      Jetpants.topology.write_config
    end
    
    
    desc 'clone_slave', 'clone a standby slave'
    method_option :source, :desc => 'IP of node to clone from'
    method_option :target, :desc => 'IP of node(s) to clone to'
    def clone_slave
      output "This task clones the data set of a standby slave."
      source = ask_node('Please enter IP of node to clone from: ', options[:source])
      source.master.probe if source.master # fail early if there are any replication issues in this pool
      describe source
      # compare against pool master if available to ensure capacity matching
      if source.for_backups? && source.pool
        compare = source.master
      else
        compare = source
      end

      unless source.running?
        error "Aborting operation" unless agree "MySQL is not running on (#{source}), do you want to start it [yes/no]: "
        source.start_mysql
      end
      error "source (#{source}) does not have the MySQL process running" unless source.running?
      error "source (#{source}) is not a standby or backup slave" unless (source.is_standby? || source.for_backups?)

      output 'You may clone to particular IP address(es), or can type "spare", "auto" or "backup_spare" to claim a node from the spare pool.'
      begin
        target = options[:target] || ask('Please enter comma-separated list of targets (IPs, "spare", "auto" or "backup_spare") to clone to: ')
      end while target.strip == '' || target.split(',').length == 0

      # Array to hold all the target nodes
      targets = []

      # This serves as a whitelist of the target shortcuts currently supported
      # You can enter "spare", "standby_spare" (which is same as "spare") or "backup_spare"
      spares_needed = {'standby' => 0, 'backup' => 0}

      target.split(',').map do |t|
        t.strip!
        t.downcase!

        if is_ip? t
          target_db = t.to_db
          error "The specified target #{t.blue} is NOT a spare node" unless target_db.is_spare?
          unless target_db.probe! && target_db.usable_spare?
            error 'Aborting operation' unless agree "The target (#{t.blue}) is NOT a usable spare, do you want to use it anyway [yes/no]: "
          end
          targets.push target_db
        else
          role = t.partition('_spare').first
          if role == 'auto'
            running_standby_slaves = source.pool.running_slaves(:standby_slave).count
            if source.pool.slaves_layout[:standby_slave] > running_standby_slaves
              spares_needed['standby'] = source.pool.slaves_layout[:standby_slave] - running_standby_slaves
            end
            running_backup_slaves = source.pool.running_slaves(:backup_slave).count
            if source.pool.slaves_layout[:backup_slave] > running_backup_slaves
              spares_needed['backup'] = source.pool.slaves_layout[:backup_slave] - running_backup_slaves
            end
            output "Number of standby slaves needed: #{spares_needed['standby']}" if spares_needed['standby'] > 0
            output "Number of backup slaves needed: #{spares_needed['backup']}" if spares_needed['backup'] > 0
          else
            role = 'standby' if role == 'spare'
            error "Invalid target #{t.blue} specified" if spares_needed[role].nil?
            spares_needed[role] += 1
          end
        end
      end

      spares_needed.each do |role, needed|
        next if needed == 0
        available = Jetpants.topology.count_spares(role:  "#{role}_slave".to_sym, like: compare)
        raise "Not enough spare machines with role of #{role} slave! Requested #{needed} but only have #{available} available." if needed > available
      end

      spares_needed.each do |role, needed|
        next if needed == 0
        targets.concat Jetpants.topology.claim_spares(needed, role: "#{role}_slave".to_sym, like: compare)
      end

      targets.each do |t|
        error "You cannot clone a slave to the same node as the target" if t.ip == source.ip
        error "target #{t} already has a master; please clear out node (including in asset tracker) before proceeding" if t.master
        error "target #{t} is running a different version of MySQL than source #{source}! Cannot proceed with clone operation." if t.version_cmp(source) != 0
      end

      # claim all the targets which are currently marked as spare
      targets.select(&:is_spare?).each(&:claim!)

      source.enslave_siblings!(targets)
      source.pool.sync_configuration

      output "Cloning complete."
      Jetpants.topology.write_config
    end
    
    
    desc 'activate_slave', 'turn a standby slave into an active slave'
    method_option :node, :desc => 'IP of standby slave to activate'
    def activate_slave
      output "This task turns a standby slave into an active slave, OR alters an active slave's weight."
      node = ask_node('Please enter node IP: ', options[:node])
      describe node

      weight = options[:weight] || ask('Please enter weight, or ENTER for default of 100: ')
      weight = 100 if weight == ''
      weight = weight.to_i
      error "Adding a slave of weight 0 makes no sense, use pull_slave instead" if weight == 0
      
      node.pool.mark_slave_active(node, weight)
      Jetpants.topology.write_config
    end
    
    
    desc 'weigh_slave', 'change the weight of an active slave'
    alias :weigh_slave :activate_slave
    
    
    desc 'pull_slave', 'turn an active slave into a standby slave'
    method_option :node, :desc => 'IP of active slave to pull'
    def pull_slave
      output "This task turns an active slave into a standby slave."
      node = ask_node('Please enter node IP: ', options[:node])
      describe node
      if node.running?
        raise "Node is not an active slave" unless node.role == :active_slave
      else
        inform "Unable to connect to node #{node} to pull slave"
        error  "Unable to pull slave" unless agree "Please confirm that #{node} is offline [yes/no]: "
      end
      node.pool.mark_slave_standby(node)
      Jetpants.topology.write_config
    end
    
    
    desc 'destroy_slave', 'remove a standby slave from its pool'
    method_option :node, :desc => 'IP of standby slave to remove'
    def destroy_slave
      # Permit slaves with broken replication to be destroyed
      Jetpants.verify_replication = false
      output "This task removes a standby/backup slave from its pool entirely. THIS IS PERMANENT, ie, it does a RESET SLAVE on the target."
      node = ask_node('Please enter node IP: ', options[:node])
      describe node
      if node.running? && node.available?
        raise "Node is not a standby or backup slave" unless (node.is_standby? || node.for_backups?)
      else
        output "Please note that we cannot run a RESET SLAVE on the node, because MySQL is not running or is otherwise unreachable."
        output "If the node is not permanently dead, you will have to run this manually."
      end
      raise "Aborting" unless ask('Please type YES in all capital letters to confirm removing node from its pool: ') == 'YES'
      node.pool.remove_slave!(node)
      node.revoke_all_access! if node.running? && node.available?
    end
    
    
    desc 'defrag_slave', 'export and re-import data set on a standby slave'
    method_option :node, :desc => 'IP of standby slave to defragment'
    def defrag_slave
      output "This task exports all data on a standby/backup slave and then re-imports it."
      node = ask_node('Please enter node IP: ', options[:node])
      describe node
      raise "Node is not a standby or backup slave" unless (node.is_standby? || node.for_backups?)
      raise "Cannot defragment non-shard slaves from command suite yet; use jetpants console instead" unless node.pool.is_a?(Shard)
      node.rebuild!
    end
    
    
    desc 'shard_read_only', 'mark a shard as read-only'
    method_option :min_id, :desc => 'Minimum ID of shard to mark as read-only'
    def shard_read_only
      shard_min = options[:min_id] || ask('Please enter min ID of the shard: ')
      s = Jetpants.topology.shard shard_min
      raise "Shard not found" unless s
      s.state = :read_only
      s.sync_configuration
      Jetpants.topology.write_config
    end
    
    
    desc 'shard_offline', 'mark a shard as offline (not readable or writable)'
    method_option :min_id, :desc => 'Minimum ID of shard to mark as offline'
    def shard_offline
      shard_min = options[:min_id] || ask('Please enter min ID of the shard: ')
      s = Jetpants.topology.shard shard_min
      raise "Shard not found" unless s
      s.state = :offline
      s.sync_configuration
      Jetpants.topology.write_config
    end
    
    
    desc 'shard_online', 'mark a shard as fully online (readable and writable)'
    method_option :min_id, :desc => 'Minimum ID of shard to mark as fully online'
    def shard_online
      shard_min = options[:min_id] || ask('Please enter min ID of the shard: ')
      s = Jetpants.topology.shard shard_min
      raise "Shard not found" unless s
      s.state = :ready
      s.sync_configuration
      Jetpants.topology.write_config
    end
    
    
    desc 'shard_split', 'Shard split (step 1 of 4): spin up child pools with different portions of data set'
    method_option :min_id, :desc => 'Minimum ID of parent shard to split'
    method_option :max_id, :desc => 'Maximum ID of parent shard to split'
    method_option :ranges, :desc => 'Optional comma-separated list of ranges per child ie "1000-1999,2000-2499" (default if omitted: split evenly)'
    method_option :count,  :desc => 'How many child shards to split the parent into (only necessary if the ranges option is omitted)'
    method_option :shard_pool, :desc => 'The sharding pool for which to perform the split'
    def shard_split
      shard_min = options[:min_id] || ask('Please enter min ID of the parent shard: ')
      shard_max = options[:max_id] || ask('Please enter max ID of the parent shard: ')

      shard_pool = options[:shard_pool] || ask("Please enter the sharding pool for which to perform the split (enter for default pool, #{Jetpants.topology.default_shard_pool}): ")
      shard_pool = Jetpants.topology.default_shard_pool if shard_pool.empty?

      output "Using shard pool `#{shard_pool}`"

      s = Jetpants.topology.shard(shard_min, shard_max, shard_pool)
      
      raise "Shard not found" unless s
      raise "Shard isn't in ready state" unless s.state == :ready
      raise "Cannot split the last shard (max ID of infinity), use \"jetpants shard_cutover\" instead" if s.max_id == 'INFINITY'
      
      ranges = options[:ranges] || ask('Optionally enter comma-separated list of ranges per child ie "1000-1999,2000-2499" [default=even split]: ')
      ranges = false if ranges.strip == ''
      
      if ranges
        supply_ranges = []
        ranges.split(',').each do |my_range|
          my_range.strip!
          my_min, my_max = my_range.split('-', 2)
          my_min = my_min.strip.to_i
          my_max = my_max.strip.to_i
          raise "Supplied range list has gaps!" if supply_ranges.last && supply_ranges.last[1] + 1 != my_min
          supply_ranges << [my_min, my_max]
        end
        children = supply_ranges.count
        raise "Supplied range does not cover parent completely!" if supply_ranges.first[0] != shard_min.to_i || supply_ranges.last[1] != shard_max.to_i
        s.split!(children, supply_ranges)
      else
        children = options[:count] || ask('Optionally enter how many children to split into [default=2]: ')
        children = 2 if children == ''
        s.split!(children.to_i)
      end
    end
    def self.before_shard_split
      reminders(
        'This process may take several hours. You probably want to run this from a screen session.',
        'Be especially careful if you are relying on SSH Agent Forwarding for your root key, since this is not screen-friendly.'
      )
    end
    def self.after_shard_split
      reminders(
        'Proceed to next step: jetpants shard_split_child_reads'
      )
    end
    
    
    # This step is only really necessary if asset-tracker changes don't immediately reflect in application configuration.
    # (ie, if app configuration is a static file that needs to be deployed to webs.)
    desc 'shard_split_child_reads', 'Shard split (step 2 of 4): move reads to child shards'
    def shard_split_child_reads
      s = ask_shard_being_split
      s.move_reads_to_children
      Jetpants.topology.write_config
    end
    def self.after_shard_split_child_reads
      reminders(
        'Commit/push the configuration in version control.',
        'Deploy the configuration to all machines.',
        'Wait for reads to stop on the old parent master.',
        'Proceed to next step: jetpants shard_split_child_writes'
      )
    end
    
    
    desc 'shard_split_child_writes', 'Shard split (step 3 of 4): move writes to child shards'
    method_option :min_id, :desc => 'Minimum ID of parent shard being split'
    method_option :max_id, :desc => 'Maximum ID of parent shard being split'
    def shard_split_child_writes
      s = ask_shard_being_split
      s.move_writes_to_children
      Jetpants.topology.write_config
    end
    def self.after_shard_split_child_writes
      reminders(
        'Commit/push the configuration in version control.',
        'Deploy the configuration to all machines.',
        'Wait for writes to stop on the old parent master.',
        'Proceed to next step: jetpants shard_split_cleanup'
      )
    end
    
    
    desc 'shard_split_cleanup', 'Shard split (step 4 of 4): clean up data that replicated to wrong shard'
    method_option :min_id, :desc => 'Minimum ID of parent shard being split'
    method_option :max_id, :desc => 'Maximum ID of parent shard being split'
    def shard_split_cleanup
      s = ask_shard_being_split
      s.cleanup!
    end
    def self.after_shard_split_cleanup
      reminders(
        'Review old nodes for hardware issues before re-using, or simply cancel them.',
      )
    end
    
    
    desc 'shard_cutover', 'truncate the current last shard range, and add a new shard after it'
    method_option :cutover_id, :desc => 'Minimum ID of new last shard being created'
    method_option :shard_pool, :desc => 'The sharding pool for which to perform the cutover'
    def shard_cutover
      cutover_id = options[:cutover_id] || ask('Please enter min ID of the new shard to be created: ')
      cutover_id = cutover_id.to_i
      shard_pool = options[:shard_pool] || ask("Please enter the sharding pool for which to perform the split (enter for default pool, #{Jetpants.topology.default_shard_pool}): ")
      shard_pool = Jetpants.topology.default_shard_pool if shard_pool.empty?

      last_shard = Jetpants.topology.shards(shard_pool).select {|s| s.max_id == 'INFINITY' && s.in_config?}.first
      last_shard_master = last_shard.master
      
      # Simple sanity-check that the cutover ID is greater than the current last shard's MIN id.
      # (in a later release, this may be improved to also ensure it's greater than any sharding
      # key value in use on the last shard.)
      raise "Specified cutover ID is too low!" unless cutover_id > last_shard.min_id
      
      # Ensure enough spare nodes are available before beginning.
      # We supply the *previous* last shard as context for counting spares
      # because the new last shard can't be created yet (chicken-and-egg problem -- master
      # must exist before we create the pool). The assumption is the hardware spec
      # of the new last shard and previous last shard will be the same.
      raise "Not enough total spare machines!" unless Jetpants.topology.count_spares(like: last_shard_master) >= last_shard.slaves_layout[:standby_slave] + last_shard.slaves_layout[:backup_slave] + 1
      raise "Not enough standby_slave role spare machines!" unless Jetpants.topology.count_spares(role: :standby_slave, like: last_shard_master) >= last_shard.slaves_layout[:standby_slave]
      raise "Not enough backup_slave role spare machines!" unless Jetpants.topology.count_spares(role: :backup_slave) >= last_shard.slaves_layout[:backup_slave]
      raise "Cannot find a spare master-role machine!" unless Jetpants.topology.count_spares(role: :master, like: last_shard_master) >= 1
      
      # In asset tracker, remove the last shard pool and replace it with a new pool. The new pool
      # has the same master/slaves but now has a non-infinity max ID.
      last_shard.state = :recycle
      last_shard.sync_configuration
      last_shard_replace = Shard.new(last_shard.min_id, cutover_id - 1, last_shard_master, :ready, shard_pool)
      last_shard_replace.sync_configuration
      Jetpants.topology.add_pool last_shard_replace
      
      # Now put another new shard after that one. (See earlier comment as to why we're supplying
      # the pool from the old last shard. This param is just used to select the right type of hardware,
      # NOT to actually set the pool of the returned object.)
      new_last_shard_master = Jetpants.topology.claim_spare(role: :master, like: last_shard_master)
      new_last_shard_master.disable_read_only! if new_last_shard_master.running?
      if last_shard.slaves_layout[:standby_slave] > 0
        # Verify spare count again, now that we can actually supply the new master as the :like context
        raise "Not enough standby_slave role spare machines!" unless Jetpants.topology.count_spares(role: :standby_slave, like: new_last_shard_master) >= last_shard.slaves_layout[:standby_slave]
        new_last_shard_slaves = Jetpants.topology.claim_spares(last_shard.slaves_layout[:standby_slave], role: :standby_slave, like: new_last_shard_master, for_pool: last_shard_replace)
        new_last_shard_slaves.each do |x| 
          x.change_master_to new_last_shard_master
          x.resume_replication
        end
      end

      # Set up backup slaves
      if last_shard.slaves_layout[:backup_slave] > 0
        raise "Not enough backup_slave role spare machines!" unless Jetpants.topology.count_spares(role: :backup_slave) >= last_shard.slaves_layout[:backup_slave]
        new_last_shard_backup_slaves = Jetpants.topology.claim_spares(last_shard.slaves_layout[:backup_slave], role: :backup_slave, for_pool: last_shard_replace)
        new_last_shard_backup_slaves.each do |x| 
          x.change_master_to new_last_shard_master
          x.resume_replication
        end
      end

      new_last_shard = Shard.new(cutover_id, 'INFINITY', new_last_shard_master, :ready, shard_pool)
      new_last_shard.sync_configuration
      Jetpants.topology.add_pool new_last_shard
      
      # Create tables on the new shard's master, obtaining schema from previous last shard
      tables = Table.from_config('sharded_tables', shard_pool)
      last_shard_master.export_schemata tables
      last_shard_master.host.fast_copy_chain(Jetpants.export_location, new_last_shard_master.host, files: ["create_tables_#{last_shard_master.port}.sql"])
      new_last_shard_master.import_schemata!
      
      # regen config file
      Jetpants.topology.write_config
    end
    def self.after_shard_cutover
      reminders(
        'Commit/push the configuration in version control.',
        'Deploy the configuration to all machines.',
      )
    end

    desc 'shard_promote_master', 'Lockless shard master promotion (step 1 of 4): initialize branched replication'
    method_option :min_id, :desc => 'Minimum ID of shard involved in master promotion'
    method_option :max_id, :desc => 'Maximum ID of shard involved in master promotion'
    method_option :new_master, :desc => 'New node to become master of the shard'
    method_option :shard_pool, :desc => 'The sharding pool for which to perform the promotion'
    def shard_promote_master
      shard_pool = options[:shard_pool] || ask("Please enter the sharding pool for which to perform the master promotion (enter for default, #{Jetpants.topology.default_shard_pool}): ")
      shard_pool = Jetpants.topology.default_shard_pool if shard_pool.empty?
      # find the shard we are going to do master promotion on
      s = ask_shard_being_promoted(:prep, options[:max_id], options[:max_id], shard_pool)

      new_master = ask_node "Please enter the IP of the new master for #{s}: ", options[:new_master]
      raise "New master node #{new_master} is not currently a slave in shard #{s}" unless s.slaves && s.slaves.include?(new_master)

      targets = s.slaves.reject { |slave| slave == new_master }
      new_master.pause_replication_with *targets
      targets.concurrent_each do |slave|
        slave.change_master_to new_master
        slave.resume_replication
        slave.catch_up_to_master
      end
      new_master.resume_replication
      new_master.catch_up_to_master

      # sync the shard configuration
      s.sync_configuration
    end
    def self.after_shard_promote_master
      reminders(
        'Proceed to next step: jetpants shard_promote_master_reads'
      )
    end

    desc 'shard_promote_master_reads', 'Lockless shard master promotion (step 2 of 4): move reads to new master'
    method_option :min_id, :desc => 'Minimum ID of shard involved in master promotion'
    method_option :max_id, :desc => 'Maximum ID of shard involved in master promotion'
    method_option :shard_pool, :desc => 'The sharding pool for which to perform the promotion'
    def shard_promote_master_reads
      shard_pool = options[:shard_pool] || ask("Please enter the sharding pool for which to perform the master promotion (enter for default pool, #{Jetpants.topology.default_shard_pool}): ")
      shard_pool = Jetpants.topology.default_shard_pool if shard_pool.empty?
      # find the shard we are going to do master promotion on
      s = ask_shard_being_promoted(:prep, options[:max_id], options[:max_id], shard_pool)

      # at this point we only have one slave, which is the new master
      new_master = s.master.slaves.last

      raise "Shard #{s} is in wrong state to perform this action! Expected :ready, found #{s.state}" unless s.state == :ready
      raise "New master node #{new_master} is not currently a slave in shard #{s}" unless s.slaves.include? new_master
      raise "New master node #{new_master} does not have any slaves" unless new_master.slaves

      s.change_master_to! new_master
      s.state = :child
      s.sync_configuration

      Jetpants.topology.write_config
    end
    def self.after_shard_promote_master_reads
      reminders(
        'Commit/push the configuration in version control.',
        'Deploy the configuration to all machines.',
        'Wait for reads to stop on the old master.',
        'Proceed to next step: jetpants shard_promote_master_writes'
      )
    end

    desc 'shard_promote_master_writes', 'Lockless shard master promotion (step 3 of 4): move writes to new master'
    method_option :shard_pool, :desc => 'The sharding pool for which to perform the promotion'
    def shard_promote_master_writes
      shard_pool = options[:shard_pool] || ask("Please enter the sharding pool for which to perform the master promotion (enter for default pool, #{Jetpants.topology.default_shard_pool}): ")
      shard_pool = Jetpants.topology.default_shard_pool if shard_pool.empty?
      s = ask_shard_being_promoted(:writes, nil, nil, shard_pool)
      if s.state != :child
        raise "Shard #{s} is in wrong state to perform this action! Expected :child, found #{s.state}"
      end

      s.master.disable_read_only!
      s.state = :needs_cleanup
      s.sync_configuration

      Jetpants.topology.write_config
    end
    def self.after_shard_promote_master_writes
      reminders(
        'Commit/push the configuration in version control.',
        'Deploy the configuration to all machines.',
        'Wait for writes to stop on the old master.',
        'Proceed to next step: jetpants shard_promote_master_cleanup'
      )
    end

    desc 'shard_promote_master_cleanup', 'Lockless shard master promotion (step 4 of 4): clean up shard and eject old master'
    method_option :shard_pool, :desc => 'The sharding pool for which to perform the promotion'
    def shard_promote_master_cleanup
      shard_pool = options[:shard_pool] || ask("Please enter the sharding pool for which to perform the master promotion (enter for default pool, #{Jetpants.topology.default_shard_pool}): ")
      shard_pool = Jetpants.topology.default_shard_pool if shard_pool.empty?
      s = ask_shard_being_promoted(:cleanup, nil, nil, shard_pool)
      if s.state != :needs_cleanup
        raise "Shard #{s} is in wrong state to perform this action! Expected :needs_cleanup, found #{s.state}"
      end

      s.cleanup!
    end

    desc 'rebalance_backup_slaves', 'Add backup slaves to pools which contain too few (does not destroy existing slaves)'
    def rebalance_backup_slaves
      spares_available = Jetpants.topology.count_spares(role: :backup_slave)

      raise "No backup slaves available" if spares_available == 0

      # find shards that need a backup slave
      need_backup_shards = Jetpants.topology.shards.reject { |shard| shard.backup_slaves.count >= shard.slaves_layout[:backup_slave] }

      possible_iterations = spares_available < need_backup_shards.count ? spares_available : need_backup_shards.count

      spares = Jetpants.topology.claim_spares(possible_iterations, role: :backup_slave)

      need_backup_shards.first(possible_iterations)

      # loop through and place a backup slave per pool that is without
      need_backup_shards.limited_concurrent_map(5) do |shard|

        if spares.count > 0

          output "Considering shard #{shard}"

          source = shard.standby_slaves.last
          source.master.probe if source.master # fail early if there are any replication issues in this pool


          targets = [spares.pop]

          source.start_mysql unless source.running?
          error "Source (#{source}) is not a standby slave" unless source.is_standby?

          targets.each do |t|
            error "Target #{t} already has a master; please clear out node (including in asset tracker) before proceeding" if t.master
            error "Target #{t} is running a different version of MySQL than source #{source}! Cannot proceed with clone operation." if t.version_cmp(source) != 0 
            error "Target #{t} already has a pool!" if t.pool
          end

          source.enslave_siblings!(targets)
          targets.concurrent_each {|t| t.resume_replication; t.catch_up_to_master(21600)}
          source.pool.sync_configuration
          output "Rebalance complete for #{shard}"

        end

      end
    end

   # This is particularly useful during merge, split, activate_slave, pull_slave operations to keep an eye on the commands
    desc 'watch_commands', 'Outputs number of SELECT and INSERT commands executed on each node in the list'
    def watch_commands
      targets = ask('Please enter comma-separated list of nodes to watch(IPs or Hostnames): ')
      nodes_array = []
      targets.chomp.split(',').map do |target|
        if target.to_db && target.to_db.running?
          nodes_array << target.to_db
        else
          output "Node: #{target} is not valid or not running mysql"
        end
      end

      if nodes_array.empty?
        output "No nodes to watch, exiting."
        exit
      end

      output "Press Ctrl + C to quit"

      header = " %10s " % [" "]
      nodes_array.each do |node|
        header += " %30s " % [node.ip]
      end

      Signal.trap("INT") {
        output "Stopping the watch"
        output header
        get_query_counts(nodes_array)
        exit
      }

      count = 0
      loop do
        output header if count % 20 == 0
        get_query_counts(nodes_array)
        sleep(1)
        count += 1
      end
    end

    no_tasks do
      def to_s
        'console'
      end

      def is_ip?(address)
        address =~ /(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})/
      end

      def error(message)
        output ['ERROR:'.red, message].join(' '), :error
        abort
      end

      def inform(message)
        output message.blue, :warn
      end

      def output(str = "\s", level = :info)
        puts str
      end

      def describe(node)
        output "Node #{node} (#{node.hostname}:#{node.port}) has role #{node.role} in pool #{node.pool(true)}.".green
      end

      def ask_node(prompt, supplied_node=false)
        node = supplied_node || ask(prompt)
        error "Node (#{node.blue}) does not appear to be an IP address." unless is_ip? node
        node.to_db
      end

      def ask_shard(shards, prompt)
        output prompt
        shards.each { |sbs| output "* #{sbs}" }
        output 'Which shard would you like to perform this action on?'
        shard_min = ask('Please enter min ID of the shard: ')
        shard_max = ask('Please enter max ID of the shard: ')
        shard_pool = ask("Please enter the sharding pool which to perform the action on (enter for default pool, #{Jetpants.topology.default_shard_pool}): ")
        shard_pool = Jetpants.topology.default_shard_pool if shard_pool.empty?
        s = Jetpants.topology.shard(shard_min, shard_max, shard_pool)
        raise 'Shard not found' unless s
        s
      end

      def ask_shard_being_split
        shard_pool = ask("Enter shard pool to take action on (enter for default pool, #{Jetpants.topology.default_shard_pool}):")
        shard_pool = Jetpants.topology.default_shard_pool if shard_pool.empty?
        shards_being_split = Jetpants.shards(shard_pool).select {|s| s.children.count > 0}
        if shards_being_split.count == 0
          raise 'No shards are currently being split. You can only use this task after running "jetpants shard_split".'
        elsif shards_being_split.count == 1
          s = shards_being_split[0]
          output "Detected #{s} as the only shard currently involved in a split operation."
          error "Aborting." unless agree "Is this the right shard that you want to perform this action on? [yes/no]: "
        else
          s = ask_shard(shards_being_split, 'The following shards are currently involved in a split operation:')
        end
        raise "Shard does not have children" if (s.children.nil? or s.children.count ==0)
        s
      end

      def ask_shard_being_promoted(stage = :prep, min_id = nil, max_id = nil, shard_pool)
        if stage == :writes || stage == :cleanup
          shards_being_promoted = Jetpants.shards(shard_pool).select do |s|
            [:reads, :child, :needs_cleanup].include?(s.state) && !s.parent && s.master.master
          end

          if shards_being_promoted.size == 0
            raise 'No shards are currently involved in a master promotion. You can only use this task after running "jetpants shard_promote_master".'
          elsif shards_being_promoted.size == 1
            s = shards_being_promoted.first
            puts "Detected #{s} as the only shard currently involved in a master promotion."
            error 'Aborting.' unless agree "Is this the right shard that you want to perform this action on? [yes/no]: "
          else
            s = ask_shard(shards_being_promoted, "The following shards are already involved in a master promotion:")
          end
        elsif min_id and max_id
          s = Jetpants.topology.shard(min_id, max_id)
        else
          min_id = ask("Enter min id of shard to perform mater promotion: ").to_i
          max_id = ask("Enter max id of shard to perform master promotion: ")
          max_id = Integer(max_id) rescue max_id.upcase

          s = Jetpants.topology.shard(min_id, max_id, shard_pool)
        end
        raise "Invalid shard selected!" unless s.is_a? Shard

        s
      end

      def get_query_counts(nodes_array)
        op_line = ''
        nodes_array.map do |node|
          begin
            selects = node.query_return_array("SHOW GLOBAL STATUS LIKE 'Com_select%'").first[:Value]
            inserts = node.query_return_array("SHOW GLOBAL STATUS LIKE 'Com_insert%'").first[:Value]
          rescue => ex
            node.output "Failed to get command status: #{ex.message}"
            selects = "N/A"
            inserts = "N/A"
          end

          op_line += " S:%12s I:%12s |" % [selects, inserts]
        end
        output op_line
      end
    end

    def self.reminders(*strings)
      strings.map! {|s| "  - #{s}"}
      strings.flatten!
      reminder_text = strings.join "\n"
      noun = (strings.count == 1 ? 'REMINDER' : 'REMINDERS')
      puts "\n#{noun}:\n#{reminder_text}\n\n" if strings.count > 0
    end
  end
end

# We load jetpants last so that plugins can monkeypatch Jetpants::CommandSuite if desired.
require 'jetpants'

def with_debug
  Pry::rescue do
    begin
      yield if block_given?
    rescue => e
      Pry.config.prompt = proc {|_, _, _| "# debug > "}
      puts "Entering debugging session (debug_exceptions is enabled).  Resume with ctrl+d."
      Pry::rescued e
    end
  end
end

if Jetpants.debug_exceptions
  with_debug { Jetpants::CommandSuite.start }
else
  Jetpants::CommandSuite.start
end
